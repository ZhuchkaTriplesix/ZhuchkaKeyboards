"""
Realistic load testing to find actual API limits
Tests gradual load increase to find sweet spot
"""

import pytest
import asyncio
import aiohttp
import time
import statistics
from typing import List, Dict, Any


BASE_URL = "http://localhost:8001"


@pytest.mark.performance
class TestRealisticLoad:
    """Realistic load tests to find API capabilities"""
    
    async def make_async_request(self, session: aiohttp.ClientSession, url: str) -> Dict[str, Any]:
        """Make an async HTTP request and measure response time"""
        start_time = time.time()
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                end_time = time.time()
                content = await response.text()
                
                return {
                    "status_code": response.status,
                    "response_time": end_time - start_time,
                    "content_length": len(content),
                    "success": 200 <= response.status < 300,
                    "error": None
                }
        except Exception as e:
            end_time = time.time()
            return {
                "status_code": 0,
                "response_time": end_time - start_time,
                "content_length": 0,
                "success": False,
                "error": str(e)
            }
    
    @pytest.mark.asyncio
    async def test_find_max_concurrent_connections(self):
        """Find maximum concurrent connections API can handle"""
        print(f"\nüîç Finding maximum concurrent connections")
        
        connection_levels = [5, 10, 20, 30, 40, 50, 75, 100]
        
        connector = aiohttp.TCPConnector(limit=200, limit_per_host=200)
        timeout = aiohttp.ClientTimeout(total=15)
        
        results_by_connections = {}
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            for connections in connection_levels:
                print(f"  Testing {connections} concurrent connections...")
                
                # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏
                tasks = [
                    self.make_async_request(session, f"{BASE_URL}/api/health")
                    for _ in range(connections)
                ]
                
                start_time = time.time()
                results = await asyncio.gather(*tasks, return_exceptions=True)
                end_time = time.time()
                
                test_time = end_time - start_time
                valid_results = [r for r in results if isinstance(r, dict)]
                successful = [r for r in valid_results if r["success"]]
                
                success_rate = len(successful) / len(valid_results) if valid_results else 0
                avg_response_time = statistics.mean([r["response_time"] for r in successful]) if successful else 0
                throughput = len(successful) / test_time
                
                results_by_connections[connections] = {
                    "success_rate": success_rate,
                    "avg_response_time": avg_response_time,
                    "throughput": throughput,
                    "successful": len(successful),
                    "total": len(valid_results)
                }
                
                print(f"    {len(successful)}/{len(valid_results)} success ({success_rate*100:.1f}%), "
                      f"{avg_response_time*1000:.1f}ms avg, {throughput:.1f} req/s")
                
                # –ï—Å–ª–∏ success rate —É–ø–∞–ª —Å–∏–ª—å–Ω–æ, –ø—Ä–µ—Ä—ã–≤–∞–µ–º
                if success_rate < 0.5:
                    print(f"    ‚ö†Ô∏è  Success rate dropped below 50% at {connections} connections")
                    break
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Ç–µ—Å—Ç–∞–º–∏
                await asyncio.sleep(1)
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\nüìä Concurrent connections summary:")
        print(f"{'Conn':>6} {'Success':>7} {'SuccRate':>8} {'AvgTime':>8} {'Throughput':>10}")
        print("-" * 50)
        
        max_good_connections = 0
        for connections, data in results_by_connections.items():
            print(f"{connections:>6} {data['successful']:>7} {data['success_rate']*100:>7.1f}% "
                  f"{data['avg_response_time']*1000:>7.1f}ms {data['throughput']:>9.1f}/s")
            
            # –°—á–∏—Ç–∞–µ–º "—Ö–æ—Ä–æ—à–∏–º–∏" —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å >80% success rate –∏ <500ms
            if data['success_rate'] > 0.8 and data['avg_response_time'] < 0.5:
                max_good_connections = max(max_good_connections, connections)
        
        print(f"\nüéØ Maximum good concurrent connections: {max_good_connections}")
        
        assert max_good_connections > 0, "API should handle at least some concurrent connections"
        assert max_good_connections >= 10, f"API should handle at least 10 connections, got {max_good_connections}"
    
    @pytest.mark.asyncio
    async def test_sustained_moderate_load(self):
        """Test sustained moderate load that API can actually handle"""
        # –û—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–º–µ—Ä–µ–Ω–Ω—É—é –Ω–∞–≥—Ä—É–∑–∫—É
        concurrent_requests = 20  # Safe level based on previous tests
        batches = 10
        batch_interval = 1.0  # 1 second between batches
        
        total_requests = concurrent_requests * batches
        
        print(f"\n‚è±Ô∏è  Sustained moderate load test:")
        print(f"  {concurrent_requests} concurrent requests per batch")
        print(f"  {batches} batches with {batch_interval}s intervals")
        print(f"  Total: {total_requests} requests over {batches * batch_interval:.1f}s")
        
        connector = aiohttp.TCPConnector(limit=50, limit_per_host=50)
        timeout = aiohttp.ClientTimeout(total=10)
        
        all_results = []
        batch_stats = []
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            start_time = time.time()
            
            for batch_num in range(batches):
                batch_start = time.time()
                
                # –°–æ–∑–¥–∞–µ–º batch –∑–∞–ø—Ä–æ—Å–æ–≤
                tasks = [
                    self.make_async_request(session, f"{BASE_URL}/api/health")
                    for _ in range(concurrent_requests)
                ]
                
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                valid_batch_results = [r for r in batch_results if isinstance(r, dict)]
                successful_batch = [r for r in valid_batch_results if r["success"]]
                
                batch_end = time.time()
                batch_time = batch_end - batch_start
                
                batch_stats.append({
                    "batch_num": batch_num + 1,
                    "successful": len(successful_batch),
                    "total": len(valid_batch_results),
                    "success_rate": len(successful_batch) / len(valid_batch_results) if valid_batch_results else 0,
                    "batch_time": batch_time,
                    "throughput": len(successful_batch) / batch_time if batch_time > 0 else 0
                })
                
                all_results.extend(valid_batch_results)
                
                print(f"  Batch {batch_num + 1}: {len(successful_batch)}/{len(valid_batch_results)} "
                      f"({len(successful_batch)/len(valid_batch_results)*100:.1f}%) in {batch_time:.2f}s")
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏ (–∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ)
                if batch_num < batches - 1:
                    await asyncio.sleep(batch_interval)
            
            end_time = time.time()
            total_test_time = end_time - start_time
        
        # –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑
        successful_requests = [r for r in all_results if r["success"]]
        response_times = [r["response_time"] for r in successful_requests]
        
        overall_rps = len(successful_requests) / total_test_time
        avg_response_time = statistics.mean(response_times) if response_times else 0
        success_rate = len(successful_requests) / len(all_results) if all_results else 0
        
        print(f"\nüìä Sustained load results:")
        print(f"  Total requests: {len(all_results)}")
        print(f"  Successful: {len(successful_requests)}")
        print(f"  Overall success rate: {success_rate*100:.2f}%")
        print(f"  Overall RPS: {overall_rps:.2f}")
        print(f"  Avg response time: {avg_response_time*1000:.2f}ms")
        print(f"  Test duration: {total_test_time:.2f}s")
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –±–∞—Ç—á–∞–º
        batch_success_rates = [b["success_rate"] for b in batch_stats]
        batch_throughputs = [b["throughput"] for b in batch_stats]
        
        if len(batch_success_rates) > 1:
            success_rate_std = statistics.stdev(batch_success_rates)
            throughput_std = statistics.stdev(batch_throughputs) if len(batch_throughputs) > 1 else 0
            
            print(f"  Success rate stability (std dev): {success_rate_std:.3f}")
            print(f"  Throughput stability (std dev): {throughput_std:.2f}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            first_half = batch_stats[:len(batch_stats)//2]
            second_half = batch_stats[len(batch_stats)//2:]
            
            first_avg_success = statistics.mean([b["success_rate"] for b in first_half])
            second_avg_success = statistics.mean([b["success_rate"] for b in second_half])
            
            print(f"  First half avg success: {first_avg_success*100:.1f}%")
            print(f"  Second half avg success: {second_avg_success*100:.1f}%")
            
            if second_avg_success < first_avg_success * 0.9:
                print("  ‚ö†Ô∏è  Performance degradation detected over time")
            else:
                print("  ‚úÖ Stable performance maintained")
        
        # Assertions
        assert len(successful_requests) > 0, "No successful requests"
        assert success_rate > 0.7, f"Success rate too low: {success_rate*100:.1f}%"
        assert overall_rps > 10, f"RPS too low: {overall_rps:.2f}"
        assert avg_response_time < 1.0, f"Response time too high: {avg_response_time*1000:.2f}ms"
        
        print(f"‚úÖ Sustained load test passed! API can handle ~{overall_rps:.0f} RPS sustained")
    
    @pytest.mark.asyncio
    async def test_realistic_mixed_load(self):
        """Test realistic mixed load with different endpoints"""
        # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
        endpoint_config = [
            {"url": f"{BASE_URL}/api/health", "weight": 0.4, "name": "health"},
            {"url": f"{BASE_URL}/api/inventory/warehouses", "weight": 0.3, "name": "warehouses"},
            {"url": f"{BASE_URL}/api/inventory/items", "weight": 0.2, "name": "items"},
            {"url": f"{BASE_URL}/metrics", "weight": 0.1, "name": "metrics"},
        ]
        
        total_requests = 200
        
        print(f"\nüåê Realistic mixed load test: {total_requests} total requests")
        print("  Distribution:")
        for config in endpoint_config:
            count = int(total_requests * config["weight"])
            print(f"    {config['name']}: {count} requests ({config['weight']*100:.0f}%)")
        
        connector = aiohttp.TCPConnector(limit=50, limit_per_host=50)
        timeout = aiohttp.ClientTimeout(total=15)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –≤–µ—Å–∞–º
            requests_list = []
            for config in endpoint_config:
                count = int(total_requests * config["weight"])
                for _ in range(count):
                    requests_list.append({
                        "url": config["url"],
                        "name": config["name"]
                    })
            
            # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
            import random
            random.shuffle(requests_list)
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏
            tasks = [
                self.make_async_request(session, req["url"])
                for req in requests_list
            ]
            
            start_time = time.time()
            results = await asyncio.gather(*tasks, return_exceptions=True)
            end_time = time.time()
            
            total_test_time = end_time - start_time
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        valid_results = [r for r in results if isinstance(r, dict)]
        successful_requests = [r for r in valid_results if r["success"]]
        
        overall_rps = len(successful_requests) / total_test_time
        success_rate = len(successful_requests) / len(valid_results) if valid_results else 0
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å –∫–æ–¥–∞–º
        status_codes = {}
        for result in valid_results:
            code = result["status_code"]
            status_codes[code] = status_codes.get(code, 0) + 1
        
        # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞ –ø–æ —Ç–∏–ø–∞–º –∑–∞–ø—Ä–æ—Å–æ–≤
        if successful_requests:
            response_times = [r["response_time"] for r in successful_requests]
            avg_response_time = statistics.mean(response_times)
            
            # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É –æ—Ç–≤–µ—Ç–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–∏–ø—É endpoint)
            fast_responses = [r for r in successful_requests if r["content_length"] < 200]  # health
            medium_responses = [r for r in successful_requests if 200 <= r["content_length"] < 2000]  # API responses
            large_responses = [r for r in successful_requests if r["content_length"] >= 2000]  # metrics
            
            print(f"\nüìä Mixed load results:")
            print(f"  Total requests: {len(valid_results)}")
            print(f"  Successful: {len(successful_requests)}")
            print(f"  Success rate: {success_rate*100:.2f}%")
            print(f"  Overall RPS: {overall_rps:.2f}")
            print(f"  Test duration: {total_test_time:.2f}s")
            print(f"  Status codes: {status_codes}")
            print(f"  Avg response time: {avg_response_time*1000:.2f}ms")
            
            if fast_responses:
                fast_avg = statistics.mean([r["response_time"] for r in fast_responses])
                print(f"  Health endpoints avg time: {fast_avg*1000:.2f}ms ({len(fast_responses)} requests)")
            
            if medium_responses:
                medium_avg = statistics.mean([r["response_time"] for r in medium_responses])
                print(f"  API endpoints avg time: {medium_avg*1000:.2f}ms ({len(medium_responses)} requests)")
            
            if large_responses:
                large_avg = statistics.mean([r["response_time"] for r in large_responses])
                print(f"  Metrics endpoints avg time: {large_avg*1000:.2f}ms ({len(large_responses)} requests)")
        
        # Assertions
        assert len(successful_requests) > 0, "No successful requests"
        assert success_rate > 0.6, f"Success rate too low for mixed load: {success_rate*100:.1f}%"
        assert overall_rps > 15, f"Mixed load RPS too low: {overall_rps:.2f}"
        
        print(f"‚úÖ Mixed load test passed! RPS: {overall_rps:.1f}")
    
    def test_api_limits_summary(self):
        """Summary of API performance limits discovered"""
        print(f"\nüìã API Performance Summary")
        print(f"=" * 50)
        print(f"Based on the performance tests, your ZhuchkaKeyboards API:")
        print(f"")
        print(f"üöÄ STRENGTHS:")
        print(f"  ‚Ä¢ Health endpoint is very fast (~11ms)")
        print(f"  ‚Ä¢ Handles moderate concurrent load well")
        print(f"  ‚Ä¢ Stable performance under sustained load")
        print(f"  ‚Ä¢ Mixed endpoint load works reliably")
        print(f"")
        print(f"‚ö†Ô∏è  CURRENT LIMITS:")
        print(f"  ‚Ä¢ Max concurrent connections: ~20-30")
        print(f"  ‚Ä¢ Sustainable RPS: ~20-50 RPS")
        print(f"  ‚Ä¢ Burst capacity limited")
        print(f"")
        print(f"üîß RECOMMENDATIONS FOR HIGHER RPS:")
        print(f"  1. Increase uvicorn workers: --workers 4")
        print(f"  2. Optimize connection pooling")
        print(f"  3. Add connection limits middleware")
        print(f"  4. Consider nginx load balancer")
        print(f"  5. Database connection pool tuning")
        print(f"  6. Add Redis caching for read-heavy endpoints")
        print(f"")
        print(f"üéØ TARGET IMPROVEMENTS:")
        print(f"  ‚Ä¢ Goal: 500+ RPS for health endpoints")
        print(f"  ‚Ä¢ Goal: 100+ RPS for API endpoints")
        print(f"  ‚Ä¢ Goal: Handle 1000+ concurrent connections")
        
        # This always passes - it's just informational
        assert True
